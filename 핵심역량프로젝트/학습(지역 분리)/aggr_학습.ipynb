{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "출처\n",
       "부산광역시_금정구      118\n",
       "서울특별시_영등포구      73\n",
       "경상북도_영덕군        49\n",
       "강원특별자치도_강릉시     34\n",
       "충청남도_청양군        30\n",
       "강원특별자치도_고성군     30\n",
       "충청남도_부여군        26\n",
       "경상북도_청송군        26\n",
       "광주광역시_동구        21\n",
       "제주도_제주시         21\n",
       "대구광역시_달서구       19\n",
       "강원특별자치도_태백시     19\n",
       "경상북도_고령군        15\n",
       "제주도_서귀포시        14\n",
       "경상북도_영양군         9\n",
       "경상북도_울릉군         8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data = pd.read_csv('0727_masking_labeling_data.csv')\n",
    "\n",
    "selected_columns1 = ['title_morphed','text_morphed', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원','출처']\n",
    "data1 = data[selected_columns1][data.aggr == 0]\n",
    "data2 = data[selected_columns1][data.aggr == 1]\n",
    "data1 = data1[300:812]\n",
    "data2 = data2\n",
    "\n",
    "data2['출처'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer 마스킹 X, 파라미터 default - best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training extra_trees...\n",
      "extra_trees - X_train shape: (906, 12176)\n",
      "extra_trees - X_test shape: (118, 12176)\n",
      "extra_trees Train set score: 0.9989\n",
      "extra_trees Test set score: 0.7034\n",
      "extra_trees Cross-validation scores: [0.76373626 0.86187845 0.86740331 0.82872928 0.71270718]\n",
      "extra_trees Cross-validation mean score: 0.8069\n",
      "\n",
      "Training ridge_classifier...\n",
      "ridge_classifier - X_train shape: (906, 12176)\n",
      "ridge_classifier - X_test shape: (118, 12176)\n",
      "ridge_classifier Train set score: 0.9967\n",
      "ridge_classifier Test set score: 0.7373\n",
      "ridge_classifier Cross-validation scores: [0.70879121 0.80662983 0.90055249 0.86187845 0.79005525]\n",
      "ridge_classifier Cross-validation mean score: 0.8136\n",
      "\n",
      "Training logistic_regression...\n",
      "logistic_regression - X_train shape: (906, 12176)\n",
      "logistic_regression - X_test shape: (118, 12176)\n",
      "logistic_regression Train set score: 0.9525\n",
      "logistic_regression Test set score: 0.6441\n",
      "logistic_regression Cross-validation scores: [0.7032967  0.84530387 0.8839779  0.87292818 0.78453039]\n",
      "logistic_regression Cross-validation mean score: 0.8180\n",
      "\n",
      "Training naive_bayes...\n",
      "naive_bayes - X_train shape: (906, 12176)\n",
      "naive_bayes - X_test shape: (118, 12176)\n",
      "naive_bayes Train set score: 0.9680\n",
      "naive_bayes Test set score: 0.7542\n",
      "naive_bayes Cross-validation scores: [0.71978022 0.75690608 0.82872928 0.82872928 0.74585635]\n",
      "naive_bayes Cross-validation mean score: 0.7760\n",
      "\n",
      "Training svm...\n",
      "svm - X_train shape: (906, 12176)\n",
      "svm - X_test shape: (118, 12176)\n",
      "svm Train set score: 0.9967\n",
      "svm Test set score: 0.6864\n",
      "svm Cross-validation scores: [0.6978022  0.8121547  0.8839779  0.8839779  0.75690608]\n",
      "svm Cross-validation mean score: 0.8070\n",
      "\n",
      "Training hard_model...\n",
      "hard_model - X_train shape: (906, 12176)\n",
      "hard_model - X_test shape: (118, 12176)\n",
      "hard_model Train set score: 0.9967\n",
      "hard_model Test set score: 0.6864\n",
      "hard_model Cross-validation scores: [0.71428571 0.82320442 0.87845304 0.87292818 0.7679558 ]\n",
      "hard_model Cross-validation mean score: 0.8114\n",
      "\n",
      "Training soft_model...\n",
      "soft_model - X_train shape: (906, 12176)\n",
      "soft_model - X_test shape: (118, 12176)\n",
      "soft_model Train set score: 0.9967\n",
      "soft_model Test set score: 0.7373\n",
      "soft_model Cross-validation scores: [0.73626374 0.83977901 0.8839779  0.86740331 0.79558011]\n",
      "soft_model Cross-validation mean score: 0.8246\n",
      "\n",
      "All models have been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data = pd.read_csv('0727_masking_labeling_data.csv')\n",
    "\n",
    "selected_columns1 = ['title_morphed','text_morphed', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원','출처']\n",
    "data1 = data[selected_columns1][data.aggr == 0]\n",
    "data2 = data[selected_columns1][data.aggr == 1]\n",
    "data1 = data1[300:812]\n",
    "data2 = data2\n",
    "\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "# 텍스트와 라벨 분리\n",
    "X_train = data3[data3.출처 != '부산광역시_금정구']['title_morphed'] + ' ' + data3[data3.출처 != '부산광역시_금정구']['text_morphed']\n",
    "X_test = data3[data3.출처 == '부산광역시_금정구']['title_morphed'] + ' ' + data3[data3.출처 == '부산광역시_금정구']['text_morphed']\n",
    "# 원래의 'text_morphed_'와 'title' 컬럼 삭제\n",
    "y_train = data3[data3.출처 != '부산광역시_금정구']['aggr']\n",
    "y_test = data3[data3.출처 == '부산광역시_금정구']['aggr']\n",
    "# 데이터 분할\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "\n",
    "    'extra_trees': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]),\n",
    "\n",
    "    'ridge_classifier': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ridge', RidgeClassifier())\n",
    "    ]),\n",
    "\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('nb', MultinomialNB())\n",
    "    ]),\n",
    "\n",
    "    'svm': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "\n",
    "\n",
    "    'hard_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('hard_model',VotingClassifier([('LR',LogisticRegression()),('ridge',RidgeClassifier()),('svm',SVC()),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='hard'))\n",
    "\n",
    "    ]),\n",
    "    'soft_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('soft_model',VotingClassifier([('LR',LogisticRegression()),('svm',SVC(probability=True)),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='soft'))\n",
    "\n",
    "    ])\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # 텍스트 데이터를 TF-IDF 벡터로 변환\n",
    "    X_train_tfidf = pipeline.named_steps['vect'].fit_transform(X_train)\n",
    "    X_test_tfidf = pipeline.named_steps['vect'].transform(X_test)\n",
    "    \n",
    "    # 차원 출력\n",
    "    print(f\"{model_name} - X_train shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"{model_name} - X_test shape: {X_test_tfidf.shape}\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # 교차검증 점수 계산\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 성능 출력\n",
    "    train_score = pipeline.score(X_train, y_train)\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    print(f\"{model_name} Train set score: {train_score:.4f}\")\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\")\n",
    "    print(f\"{model_name} Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"{model_name} Cross-validation mean score: {cv_scores.mean():.4f}\\n\")\n",
    "\n",
    "    # 모델 저장\n",
    "    # model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'0727_aggr_textmorphed_{model_name}_tfidf_nonemasking_model.pkl')\n",
    "    # joblib.dump(pipeline, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 마스킹 테스트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training extra_trees...\n",
      "extra_trees - X_train shape: (906, 11616)\n",
      "extra_trees - X_test shape: (118, 11616)\n",
      "extra_trees Train set score: 0.9989\n",
      "extra_trees Test set score: 0.7458\n",
      "extra_trees Cross-validation scores: [0.75824176 0.86187845 0.82872928 0.84530387 0.74033149]\n",
      "extra_trees Cross-validation mean score: 0.8069\n",
      "\n",
      "Training ridge_classifier...\n",
      "ridge_classifier - X_train shape: (906, 11616)\n",
      "ridge_classifier - X_test shape: (118, 11616)\n",
      "ridge_classifier Train set score: 0.9967\n",
      "ridge_classifier Test set score: 0.7373\n",
      "ridge_classifier Cross-validation scores: [0.70879121 0.80662983 0.90607735 0.83977901 0.79005525]\n",
      "ridge_classifier Cross-validation mean score: 0.8103\n",
      "\n",
      "Training logistic_regression...\n",
      "logistic_regression - X_train shape: (906, 11616)\n",
      "logistic_regression - X_test shape: (118, 11616)\n",
      "logistic_regression Train set score: 0.9481\n",
      "logistic_regression Test set score: 0.6610\n",
      "logistic_regression Cross-validation scores: [0.7032967  0.83977901 0.87292818 0.83425414 0.7679558 ]\n",
      "logistic_regression Cross-validation mean score: 0.8036\n",
      "\n",
      "Training naive_bayes...\n",
      "naive_bayes - X_train shape: (906, 11616)\n",
      "naive_bayes - X_test shape: (118, 11616)\n",
      "naive_bayes Train set score: 0.9669\n",
      "naive_bayes Test set score: 0.7288\n",
      "naive_bayes Cross-validation scores: [0.72527473 0.75138122 0.82320442 0.81767956 0.76243094]\n",
      "naive_bayes Cross-validation mean score: 0.7760\n",
      "\n",
      "Training svm...\n",
      "svm - X_train shape: (906, 11616)\n",
      "svm - X_test shape: (118, 11616)\n",
      "svm Train set score: 0.9967\n",
      "svm Test set score: 0.6780\n",
      "svm Cross-validation scores: [0.6978022  0.80662983 0.87845304 0.84530387 0.74585635]\n",
      "svm Cross-validation mean score: 0.7948\n",
      "\n",
      "Training hard_model...\n",
      "hard_model - X_train shape: (906, 11616)\n",
      "hard_model - X_test shape: (118, 11616)\n",
      "hard_model Train set score: 0.9967\n",
      "hard_model Test set score: 0.6949\n",
      "hard_model Cross-validation scores: [0.7032967  0.82872928 0.87845304 0.85082873 0.77348066]\n",
      "hard_model Cross-validation mean score: 0.8070\n",
      "\n",
      "Training soft_model...\n",
      "soft_model - X_train shape: (906, 11616)\n",
      "soft_model - X_test shape: (118, 11616)\n",
      "soft_model Train set score: 0.9967\n",
      "soft_model Test set score: 0.7288\n",
      "soft_model Cross-validation scores: [0.73076923 0.83425414 0.86740331 0.86187845 0.79558011]\n",
      "soft_model Cross-validation mean score: 0.8180\n",
      "\n",
      "All models have been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data = pd.read_csv('0727_masking_labeling_data.csv')\n",
    "\n",
    "selected_columns1 = ['title_morphed_masked','text_morphed_masked', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원','출처']\n",
    "data1 = data[selected_columns1][data.aggr == 0]\n",
    "data2 = data[selected_columns1][data.aggr == 1]\n",
    "data1 = data1[300:812]\n",
    "data2 = data2\n",
    "\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "# 텍스트와 라벨 분리\n",
    "X_train = data3[data3.출처 != '부산광역시_금정구']['title_morphed_masked'] + ' ' + data3[data3.출처 != '부산광역시_금정구']['text_morphed_masked']\n",
    "X_test = data3[data3.출처 == '부산광역시_금정구']['title_morphed_masked'] + ' ' + data3[data3.출처 == '부산광역시_금정구']['text_morphed_masked']\n",
    "# 원래의 'text_morphed_'와 'title' 컬럼 삭제\n",
    "y_train = data3[data3.출처 != '부산광역시_금정구']['aggr']\n",
    "y_test = data3[data3.출처 == '부산광역시_금정구']['aggr']\n",
    "# 데이터 분할\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "\n",
    "    'extra_trees': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]),\n",
    "\n",
    "    'ridge_classifier': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ridge', RidgeClassifier())\n",
    "    ]),\n",
    "\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('nb', MultinomialNB())\n",
    "    ]),\n",
    "\n",
    "    'svm': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "\n",
    "\n",
    "    'hard_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('hard_model',VotingClassifier([('LR',LogisticRegression()),('ridge',RidgeClassifier()),('svm',SVC()),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='hard'))\n",
    "\n",
    "    ]),\n",
    "    'soft_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('soft_model',VotingClassifier([('LR',LogisticRegression()),('svm',SVC(probability=True)),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='soft'))\n",
    "\n",
    "    ])\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # 텍스트 데이터를 TF-IDF 벡터로 변환\n",
    "    X_train_tfidf = pipeline.named_steps['vect'].fit_transform(X_train)\n",
    "    X_test_tfidf = pipeline.named_steps['vect'].transform(X_test)\n",
    "    \n",
    "    # 차원 출력\n",
    "    print(f\"{model_name} - X_train shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"{model_name} - X_test shape: {X_test_tfidf.shape}\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # 교차검증 점수 계산\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 성능 출력\n",
    "    train_score = pipeline.score(X_train, y_train)\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    print(f\"{model_name} Train set score: {train_score:.4f}\")\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\")\n",
    "    print(f\"{model_name} Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"{model_name} Cross-validation mean score: {cv_scores.mean():.4f}\\n\")\n",
    "\n",
    "    # 모델 저장\n",
    "    # model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'0727_aggr_textmorphed_{model_name}_tfidf_nonemasking_model.pkl')\n",
    "    # joblib.dump(pipeline, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer 명사제거, 마스킹 X, 파라미터 default - 0726"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training extra_trees...\n",
      "extra_trees - X_train shape: (585, 2620)\n",
      "extra_trees - X_test shape: (147, 2620)\n",
      "extra_trees Train set score: 0.9983\n",
      "extra_trees Test set score: 0.7483\n",
      "extra_trees Cross-validation scores: [0.75213675 0.74358974 0.73504274 0.7008547  0.73504274]\n",
      "extra_trees Cross-validation mean score: 0.7333\n",
      "\n",
      "Training ridge_classifier...\n",
      "ridge_classifier - X_train shape: (585, 2620)\n",
      "ridge_classifier - X_test shape: (147, 2620)\n",
      "ridge_classifier Train set score: 0.9846\n",
      "ridge_classifier Test set score: 0.8027\n",
      "ridge_classifier Cross-validation scores: [0.76923077 0.73504274 0.70940171 0.67521368 0.70940171]\n",
      "ridge_classifier Cross-validation mean score: 0.7197\n",
      "\n",
      "Training logistic_regression...\n",
      "logistic_regression - X_train shape: (585, 2620)\n",
      "logistic_regression - X_test shape: (147, 2620)\n",
      "logistic_regression Train set score: 0.9248\n",
      "logistic_regression Test set score: 0.7823\n",
      "logistic_regression Cross-validation scores: [0.74358974 0.70940171 0.73504274 0.73504274 0.70940171]\n",
      "logistic_regression Cross-validation mean score: 0.7265\n",
      "\n",
      "Training naive_bayes...\n",
      "naive_bayes - X_train shape: (585, 2620)\n",
      "naive_bayes - X_test shape: (147, 2620)\n",
      "naive_bayes Train set score: 0.9179\n",
      "naive_bayes Test set score: 0.7619\n",
      "naive_bayes Cross-validation scores: [0.72649573 0.73504274 0.69230769 0.64102564 0.65811966]\n",
      "naive_bayes Cross-validation mean score: 0.6906\n",
      "\n",
      "Training svm...\n",
      "svm - X_train shape: (585, 2620)\n",
      "svm - X_test shape: (147, 2620)\n",
      "svm Train set score: 0.9932\n",
      "svm Test set score: 0.7823\n",
      "svm Cross-validation scores: [0.75213675 0.74358974 0.72649573 0.7008547  0.7008547 ]\n",
      "svm Cross-validation mean score: 0.7248\n",
      "\n",
      "Training hard_model...\n",
      "hard_model - X_train shape: (585, 2620)\n",
      "hard_model - X_test shape: (147, 2620)\n",
      "hard_model Train set score: 0.9846\n",
      "hard_model Test set score: 0.7959\n",
      "hard_model Cross-validation scores: [0.75213675 0.73504274 0.71794872 0.70940171 0.70940171]\n",
      "hard_model Cross-validation mean score: 0.7248\n",
      "\n",
      "Training soft_model...\n",
      "soft_model - X_train shape: (585, 2620)\n",
      "soft_model - X_test shape: (147, 2620)\n",
      "soft_model Train set score: 0.9949\n",
      "soft_model Test set score: 0.8095\n",
      "soft_model Cross-validation scores: [0.75213675 0.75213675 0.72649573 0.71794872 0.70940171]\n",
      "soft_model Cross-validation mean score: 0.7316\n",
      "\n",
      "All models have been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data1 = pd.read_csv('0726_masking_labeling_data_0.csv')\n",
    "data2 = pd.read_csv('0726_masking_labeling_data_1.csv')\n",
    "\n",
    "data1 = data1.rename(columns={'text_remove_noun_morphed':'text_remove_noun_morphed'})\n",
    "data2 = data2.rename(columns={'text_remove_noun_morphed':'text_remove_noun_morphed'})\n",
    "\n",
    "data1 = data1.rename(columns={'저격성 민원':'저격성민원_'})\n",
    "data2 = data2.rename(columns={'저격성 민원':'저격성민원_'})\n",
    "\n",
    "selected_columns1 = ['text_remove_noun_morphed', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성민원_']\n",
    "data1 = data1[selected_columns1]\n",
    "data2 = data2[selected_columns1]\n",
    "data1 = data1[:366]\n",
    "data2 = data2[data2.aggr == 1]\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data3['text_remove_noun_morphed']\n",
    "data_labeling = data3['aggr']\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_contents, data_labeling, stratify=data_labeling, test_size=0.2, random_state=42)\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "\n",
    "    'extra_trees': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]),\n",
    "\n",
    "    'ridge_classifier': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ridge', RidgeClassifier())\n",
    "    ]),\n",
    "\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('nb', MultinomialNB())\n",
    "    ]),\n",
    "\n",
    "    'svm': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "\n",
    "\n",
    "    'hard_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('hard_model',VotingClassifier([('LR',LogisticRegression()),('ridge',RidgeClassifier()),('svm',SVC()),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='hard'))\n",
    "\n",
    "    ]),\n",
    "\n",
    "    'soft_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('soft_model',VotingClassifier([('LR',LogisticRegression()),('svm',SVC(probability=True)),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='soft'))\n",
    "\n",
    "    ])\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # 텍스트 데이터를 TF-IDF 벡터로 변환\n",
    "    X_train_tfidf = pipeline.named_steps['vect'].fit_transform(X_train)\n",
    "    X_test_tfidf = pipeline.named_steps['vect'].transform(X_test)\n",
    "    \n",
    "    # 차원 출력\n",
    "    print(f\"{model_name} - X_train shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"{model_name} - X_test shape: {X_test_tfidf.shape}\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # 교차검증 점수 계산\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 성능 출력\n",
    "    train_score = pipeline.score(X_train, y_train)\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    print(f\"{model_name} Train set score: {train_score:.4f}\")\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\")\n",
    "    print(f\"{model_name} Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"{model_name} Cross-validation mean score: {cv_scores.mean():.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'aggr_{model_name}_tfidf_nonemasking_model.pkl')\n",
    "    joblib.dump(pipeline, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer 마스킹 X, 파라미터 default - best 그리드 서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training extra_trees...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 141\u001b[0m\n\u001b[0;32m    138\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    140\u001b[0m \u001b[38;5;66;03m# 그리드 서치 훈련\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# 최적 하이퍼파라미터 출력\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    822\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    823\u001b[0m         clone(base_estimator),\n\u001b[0;32m    824\u001b[0m         X,\n\u001b[0;32m    825\u001b[0m         y,\n\u001b[0;32m    826\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    827\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    828\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    829\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    830\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    831\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    834\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[0;32m    835\u001b[0m     )\n\u001b[0;32m    836\u001b[0m )\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data = pd.read_csv('0727_masking_labeling_data.csv')\n",
    "\n",
    "selected_columns1 = ['text_morphed','title_morphed', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원']\n",
    "data1 = data[selected_columns1][data.aggr == 0]\n",
    "data2 = data[selected_columns1][data.aggr == 1]\n",
    "data1 = data1[300:812]\n",
    "data2 = data2\n",
    "\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data3['text_morphed'] + ' ' + data3['title_morphed']\n",
    "# \n",
    "# 원래의 'text_morphed_'와 'title' 컬럼 삭제\n",
    "data_labeling = data3['aggr']\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_contents, data_labeling, stratify=data_labeling, test_size=0.2, random_state=42)\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "    'extra_trees': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]),\n",
    "    'ridge_classifier': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ridge', RidgeClassifier())\n",
    "    ]),\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('nb', MultinomialNB())\n",
    "    ]),\n",
    "    'svm': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "    'hard_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('hard_model',VotingClassifier([('LR',LogisticRegression()),('ridge',RidgeClassifier()),('svm',SVC()),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='hard'))\n",
    "\n",
    "    ]),\n",
    "\n",
    "    'soft_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('soft_model',VotingClassifier([('LR',LogisticRegression()),('svm',SVC(probability=True)),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='soft'))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "param_grids = {\n",
    "    'extra_trees': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'et__n_estimators': [100],\n",
    "        'et__max_features': ['auto', 'sqrt'],\n",
    "        'et__max_depth': [None, 50],\n",
    "    },\n",
    "    'ridge_classifier': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'ridge__alpha': [0.1, 1, 10],\n",
    "    },\n",
    "    'logistic_regression': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'lr__C': [0.1, 1, 10],\n",
    "        'lr__penalty': ['l1', 'l2'],\n",
    "        'lr__solver': ['liblinear'],\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'nb__alpha': [0.1, 1, 10],\n",
    "    },\n",
    "    'svm': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'svm__C': [0.1, 1, 10],\n",
    "        'svm__kernel': ['linear', 'rbf'],\n",
    "        'svm__gamma': ['scale'],\n",
    "    },\n",
    "    'hard_model': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'hard_model__LR__C': [0.1, 1],\n",
    "        'hard_model__ridge__alpha': [0.1, 1],\n",
    "        'hard_model__svm__C': [0.1, 1],\n",
    "    },\n",
    "    'soft_model': {\n",
    "        'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "        'vect__max_df': [0.9, 1.0],\n",
    "        'vect__min_df': [1, 2],\n",
    "        'vect__max_features': [None, 1000],\n",
    "        'soft_model__LR__C': [0.1, 1],\n",
    "        'soft_model__svm__C': [0.1, 1],\n",
    "        'soft_model__et__n_estimators': [100],\n",
    "        'soft_model__nb__alpha': [0.1, 1],\n",
    "    },\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # 그리드 서치 설정\n",
    "    param_grid = param_grids[model_name]\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    \n",
    "    # 그리드 서치 훈련\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # 최적 하이퍼파라미터 출력\n",
    "    print(f\"Best parameters for {model_name}: {grid_search.best_params_}\")\n",
    "    \n",
    "    # 최적 모델 평가\n",
    "    best_model = grid_search.best_estimator_\n",
    "    train_score = best_model.score(X_train, y_train)\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    \n",
    "    # 교차 검증 점수 계산\n",
    "    cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 성능 출력\n",
    "    print(f\"{model_name} Train set score: {train_score:.4f}\")\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\")\n",
    "    print(f\"{model_name} Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"{model_name} Cross-validation mean score: {cv_scores.mean():.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'aggr_{model_name}_tfidf_nonemasking_model.pkl')\n",
    "    joblib.dump(best_model, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
