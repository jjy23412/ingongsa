{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "요지불명\n",
       "0    360\n",
       "1      6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data1 = pd.read_csv('0726_masking_labeling_data_0.csv')\n",
    "data2 = pd.read_csv('0726_masking_labeling_data_1.csv')\n",
    "\n",
    "\n",
    "selected_columns1 = ['text_morphed', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원']\n",
    "data1 = data1[selected_columns1]\n",
    "data2 = data2[selected_columns1]\n",
    "\n",
    "data2['요지불명'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "비꼼_시비\n",
       "1    187\n",
       "0     10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training extra_trees...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:862\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ready_batches\u001b[38;5;241m.\u001b[39mget(block\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m queue\u001b[38;5;241m.\u001b[39mEmpty:\n\u001b[0;32m    864\u001b[0m     \u001b[38;5;66;03m# slice the iterator n_jobs * batchsize items at a time. If the\u001b[39;00m\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;66;03m# slice returns less than that, then the current batchsize puts\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     \u001b[38;5;66;03m# accordingly to distribute evenly the last items between all\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# workers.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\queue.py:168\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 168\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 94\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 교차검증 점수 계산\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m cv_scores \u001b[38;5;241m=\u001b[39m cross_val_score(pipeline, X_train, y_train, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 성능 출력\u001b[39;00m\n\u001b[0;32m     97\u001b[0m train_score \u001b[38;5;241m=\u001b[39m cv_scores\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# 교차검증 점수의 평균을 훈련 점수로 사용\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:515\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    513\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 515\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    516\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    517\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    518\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    519\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    520\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    521\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    522\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    523\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    524\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m    525\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    526\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    527\u001b[0m )\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:873\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    870\u001b[0m n_jobs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached_effective_n_jobs\n\u001b[0;32m    871\u001b[0m big_batch_size \u001b[38;5;241m=\u001b[39m batch_size \u001b[38;5;241m*\u001b[39m n_jobs\n\u001b[1;32m--> 873\u001b[0m islice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mislice(iterator, big_batch_size))\n\u001b[0;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(islice) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    875\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:59\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Capture the thread-local scikit-learn configuration at the time\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Parallel.__call__ is issued since the tasks can be dispatched\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# in a different thread depending on the backend and on the value of\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# pre_dispatch and n_jobs.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m---> 59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:266\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    265\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 266\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    267\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    268\u001b[0m         clone(estimator),\n\u001b[0;32m    269\u001b[0m         X,\n\u001b[0;32m    270\u001b[0m         y,\n\u001b[0;32m    271\u001b[0m         scorers,\n\u001b[0;32m    272\u001b[0m         train,\n\u001b[0;32m    273\u001b[0m         test,\n\u001b[0;32m    274\u001b[0m         verbose,\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    276\u001b[0m         fit_params,\n\u001b[0;32m    277\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    278\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    279\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    280\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    281\u001b[0m     )\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    285\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:345\u001b[0m, in \u001b[0;36m_BaseKFold.split\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    343\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(X)\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits \u001b[38;5;241m>\u001b[39m n_samples:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    346\u001b[0m         (\n\u001b[0;32m    347\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot have number of splits n_splits=\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m greater\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m than the number of samples: n_samples=\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    349\u001b[0m         )\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_splits, n_samples)\n\u001b[0;32m    350\u001b[0m     )\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups):\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m train, test\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot have number of splits n_splits=5 greater than the number of samples: n_samples=4."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data1 = pd.read_csv('0726_masking_labeling_data_0.csv')\n",
    "data2 = pd.read_csv('0726_masking_labeling_data_1.csv')\n",
    "\n",
    "data1 = data1.rename(columns={'text_morphed': 'text_morphed_'})\n",
    "data2 = data2.rename(columns={'text_morphed': 'text_morphed_'})\n",
    "\n",
    "selected_columns1 = ['title', 'text_morphed_', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원']\n",
    "\n",
    "target = selected_columns1[5]\n",
    "\n",
    "data1 = data1[selected_columns1]\n",
    "data2 = data2[selected_columns1]\n",
    "data1 = data1[300:306]  # 라벨 데이터 개수만큼 설정\n",
    "data2 = data2[data2[target] == 1]  # 라벨 컬럼 설정\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data3['text_morphed_'] + ' ' + data3['title']\n",
    "data_labeling = data3[target]  # 라벨 컬럼 설정\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_contents, data_labeling, stratify=data_labeling, test_size=0.2, random_state=42)\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "\n",
    "    'extra_trees': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]),\n",
    "\n",
    "    'ridge_classifier': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ridge', RidgeClassifier())\n",
    "    ]),\n",
    "\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('nb', MultinomialNB())\n",
    "    ]),\n",
    "\n",
    "    'svm': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('svm', SVC(probability=True))  # probability=True 설정\n",
    "    ]),\n",
    "\n",
    "    'hard_model': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('hard_model', VotingClassifier([\n",
    "            ('LR', LogisticRegression()),\n",
    "            ('ridge', RidgeClassifier()),\n",
    "            ('svm', SVC(probability=True)),  # probability=True 설정\n",
    "            ('et', ExtraTreesClassifier()),\n",
    "            ('nb', MultinomialNB())\n",
    "        ], voting='hard'))\n",
    "    ]),\n",
    "\n",
    "    'soft_model': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('soft_model', VotingClassifier([\n",
    "            ('LR', LogisticRegression()),\n",
    "            ('svm', SVC(probability=True)),  # probability=True 설정\n",
    "            ('et', ExtraTreesClassifier()),\n",
    "            ('nb', MultinomialNB())\n",
    "        ], voting='soft'))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # 교차검증 점수 계산\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "    # 성능 출력\n",
    "    train_score = cv_scores.mean()  # 교차검증 점수의 평균을 훈련 점수로 사용\n",
    "    pipeline.fit(X_train, y_train)  # 모델 훈련\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    print(f\"{model_name} Train set score: {train_score:.4f}\")\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\")\n",
    "    print(f\"{model_name} Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"{model_name} Cross-validation mean score: {cv_scores.mean():.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'{target}_{model_name}_tfidf_nonemasking_model.pkl')\n",
    "    joblib.dump(pipeline, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      안녕 하 관리자 `지역` 문화센터 인근 학교 재직 교사 `지역` 문화센터 근방 신호...\n",
       "1      안녕 하 `지역` 아이 주민 에 인도 파손 되 넘어지 경험 있 인도 파손 부분 인도...\n",
       "2      `지역` `지역` `지역` 차선 도로 파손 되 매우 크 불편 겪 사진 첨부 지하철 ...\n",
       "3      `지역` 교차로 비보 호 좌회전 신호 있 좌회전 신호일 때 좌회전 도로 횡단보도 신...\n",
       "4      안녕 하 코로나 도둑 맞 같 현실 모든 가족 너무 화 세상에 있 답답 일단 국민 신...\n",
       "                             ...                        \n",
       "192    `지역` 빌 쓰레기 처리 대하 건의 사람 알 빌라 주민 만나 옮기 주시 하 궁 나 ...\n",
       "193    추석연 휴 친정 이틀 `지역` 빌라 차이 아직 덥 때 문 열 미치 알 사람 버리 쓰...\n",
       "194                 지나 달 차례 문의 드리 아직 방지 턱 설치 안 되 있 지나 치이\n",
       "195    군수 건축 과장 안녕 하 지나 일요일 오전 군청 발주 하 `지역` 농공 단지 앞 하...\n",
       "196    군도 호선 미개 설 공 촉구 건 대하 부여군 건설과 도로계 대답 시원 보충 질문 존...\n",
       "Name: text_morphed_masked_, Length: 395, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\n",
      "classpath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\rhinoMorph/lib/rhino.jar\n",
      "JVM is already started~\n",
      "RHINO started!\n",
      "filepath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\n",
      "classpath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\rhinoMorph/lib/rhino.jar\n",
      "JVM is already started~\n",
      "RHINO started!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(morphed_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp949\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 데이터 분할\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], stratify\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# 파라미터 그리드 정의\u001b[39;00m\n\u001b[0;32m     84\u001b[0m param_grids \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvect__max_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m20000\u001b[39m, \u001b[38;5;241m30000\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m     }\n\u001b[0;32m    132\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import rhinoMorph\n",
    "\n",
    "# XGBoost, LightGBM, CatBoost import\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# 경로 설정\n",
    "data_path = 'C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/143.민원 업무 효율, 자동화를 위한 언어 AI 학습데이터/라벨링 데이터/'\n",
    "os.chdir(data_path)\n",
    "\n",
    "# 데이터 읽기\n",
    "data1 = pd.read_csv('공격성_민원_0.csv')\n",
    "data2 = pd.read_csv('공격성 민원 모음.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "data2 = data2.drop('Unnamed: 0', axis=1)\n",
    "data2.rename(columns={\"민원내용\": 'title_and_contents'}, inplace=True)\n",
    "data1.rename(columns={\"저격성 민원\": '저격성민원'}, inplace=True)\n",
    "\n",
    "# 데이터 병합\n",
    "data3 = pd.concat([data1, data2])\n",
    "data4 = data3[['title_and_contents', 'aggr']].reset_index(drop=True)\n",
    "\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data4['title_and_contents']\n",
    "data_labeling = data4['aggr']\n",
    "\n",
    "# 형태소 분석\n",
    "rn = rhinoMorph.startRhino()\n",
    "\n",
    "def morph_and_label(contents, labels):\n",
    "    location_data = pd.read_csv('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/행정구역 데이터/file/행정구역명리스트.csv')\n",
    "    location_list = list(location_data.loc[:, 'combined'])\n",
    "\n",
    "    rn = rhinoMorph.startRhino()\n",
    "\n",
    "    morphed_data_list = []\n",
    "    for data_each in data_contents:\n",
    "        morphed_data_each, poses = rhinoMorph.wholeResult_list(rn, str(data_each),pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'])\n",
    "        # print(morphed_data_each)\n",
    "        # print(poses)\n",
    "        # 형태소 분석된 데이터에서 NNP에 대해 이름 정규 표현식 적용 및 지역명 체크\n",
    "        for j, item in enumerate(poses):\n",
    "            if item == 'NNP':\n",
    "                word = morphed_data_each[j]\n",
    "                \n",
    "                # 지역명 리스트에 있는지 체크\n",
    "                if word in location_list:\n",
    "                    morphed_data_each[j] = '`지역`'\n",
    "            \n",
    "        joined_data_each = ' '.join(morphed_data_each)\n",
    "        if joined_data_each:\n",
    "            morphed_data_list.append(joined_data_each)\n",
    "        else : \n",
    "            morphed_data_list.append(' ')\n",
    "\n",
    "morphed_data = morph_and_label(data_contents, data_labeling)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "morphed_df = pd.DataFrame(morphed_data, columns=['text', 'label'])\n",
    "\n",
    "# 데이터 저장\n",
    "morphed_file = 'ratings_morphed.txt'\n",
    "morphed_df.to_csv(morphed_file, index=False, encoding='cp949')\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(morphed_file, encoding='cp949')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], stratify=data['label'], test_size=0.2)\n",
    "\n",
    "# 파라미터 그리드 정의\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'lr__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'lr__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'svm': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'svm__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'svm__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'rf__n_estimators': [50, 100, 200],\n",
    "        'rf__max_depth': [None, 10, 20, 30]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'gb__n_estimators': [50, 100, 200],\n",
    "        'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'gb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'vect__max_features': [10000, 20000, 30000]\n",
    "    },\n",
    "    'ada_boost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'ada__n_estimators': [50, 100, 200],\n",
    "        'ada__learning_rate': [0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'lgb__n_estimators': [50, 100, 200],\n",
    "        'lgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'lgb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'catboost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'cat__iterations': [50, 100, 200],\n",
    "        'cat__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'cat__depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "    'svm': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'ada_boost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('ada', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'xgboost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('xgb', xgb.XGBClassifier(eval_metric='mlogloss'))\n",
    "    ]),\n",
    "    'lightgbm': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('lgb', lgb.LGBMClassifier())\n",
    "    ]),\n",
    "    'catboost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('cat', cb.CatBoostClassifier(learning_rate=0.1, iterations=100, depth=6, verbose=0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model_save_path = 'C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/'\n",
    "os.chdir(model_save_path)\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # 그리드 서치 정의\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # 모델 훈련\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # 최적의 파라미터와 성능 출력\n",
    "    print(f\"\\n{model_name} Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 최적의 모델로 테스트 데이터 평가\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    joblib.dump(grid_search, f'{model_name}_model_grid_search.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
