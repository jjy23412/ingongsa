{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data1 = pd.read_csv('0726_masking_labeling_data_0.csv')\n",
    "data2 = pd.read_csv('0726_masking_labeling_data_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = data1.rename(columns={'text_morphed_masked':'text_morphed_masked_'})\n",
    "data2 = data2.rename(columns={'text_morphed_masked':'text_morphed_masked_'})\n",
    "data1 = data1.rename(columns={'저격성 민원':'저격성민원_'})\n",
    "data2 = data2.rename(columns={'저격성 민원':'저격성민원_'})\n",
    "selected_columns1 = ['text_morphed_masked_', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성민원_']\n",
    "data1 = data1[selected_columns1]\n",
    "data2 = data2[selected_columns1]\n",
    "# data1 = data1[:366]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 데이터 병합\n",
    "data3 = pd.concat([data1, data2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "저격성민원_\n",
       "0    229\n",
       "1    137\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2['저격성민원_'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training extra_trees...\n",
      "extra_trees - X_train shape: (114, 3397)\n",
      "extra_trees - X_test shape: (29, 3397)\n",
      "extra_trees Train set score: 1.0000\n",
      "extra_trees Test set score: 0.9655\n",
      "extra_trees Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "extra_trees Cross-validation mean score: 0.9561\n",
      "\n",
      "Training ridge_classifier...\n",
      "ridge_classifier - X_train shape: (114, 3397)\n",
      "ridge_classifier - X_test shape: (29, 3397)\n",
      "ridge_classifier Train set score: 1.0000\n",
      "ridge_classifier Test set score: 0.9655\n",
      "ridge_classifier Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "ridge_classifier Cross-validation mean score: 0.9561\n",
      "\n",
      "Training logistic_regression...\n",
      "logistic_regression - X_train shape: (114, 3397)\n",
      "logistic_regression - X_test shape: (29, 3397)\n",
      "logistic_regression Train set score: 0.9561\n",
      "logistic_regression Test set score: 0.9655\n",
      "logistic_regression Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "logistic_regression Cross-validation mean score: 0.9561\n",
      "\n",
      "Training naive_bayes...\n",
      "naive_bayes - X_train shape: (114, 3397)\n",
      "naive_bayes - X_test shape: (29, 3397)\n",
      "naive_bayes Train set score: 0.9561\n",
      "naive_bayes Test set score: 0.9655\n",
      "naive_bayes Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "naive_bayes Cross-validation mean score: 0.9561\n",
      "\n",
      "Training svm...\n",
      "svm - X_train shape: (114, 3397)\n",
      "svm - X_test shape: (29, 3397)\n",
      "svm Train set score: 0.9561\n",
      "svm Test set score: 0.9655\n",
      "svm Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "svm Cross-validation mean score: 0.9561\n",
      "\n",
      "Training hard_model...\n",
      "hard_model - X_train shape: (114, 3397)\n",
      "hard_model - X_test shape: (29, 3397)\n",
      "hard_model Train set score: 0.9561\n",
      "hard_model Test set score: 0.9655\n",
      "hard_model Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "hard_model Cross-validation mean score: 0.9561\n",
      "\n",
      "Training soft_model...\n",
      "soft_model - X_train shape: (114, 3397)\n",
      "soft_model - X_test shape: (29, 3397)\n",
      "soft_model Train set score: 1.0000\n",
      "soft_model Test set score: 0.9655\n",
      "soft_model Cross-validation scores: [0.95652174 0.95652174 0.95652174 0.95652174 0.95454545]\n",
      "soft_model Cross-validation mean score: 0.9561\n",
      "\n",
      "All models have been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import joblib\n",
    "\n",
    "\n",
    "# 데이터 불러오기\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data1 = pd.read_csv('0726_masking_labeling_data_0.csv')\n",
    "data2 = pd.read_csv('0726_masking_labeling_data_1.csv')\n",
    "\n",
    "data1 = data1.rename(columns={'text_morphed':'text_morphed_'})\n",
    "data2 = data2.rename(columns={'text_morphed':'text_morphed_'})\n",
    "\n",
    "\n",
    "selected_columns1 = ['title','text_morphed_', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성 민원']\n",
    "\n",
    "target = selected_columns1[6]\n",
    "\n",
    "data1 = data1[selected_columns1]\n",
    "data2 = data2[selected_columns1]\n",
    "data1 = data1[300:437]                    #라벨 데이터 개수만큼 설정\n",
    "data2 = data2[data2[target] == 1]   #라벨 컬럼 설정\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data3['text_morphed_'] + ' ' + data3['title']\n",
    "data_labeling = data3[f'{target}']    #라벨 컬럼 설정\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_contents, data_labeling, stratify=data_labeling, test_size=0.2, random_state=42)\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "\n",
    "    'extra_trees': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('et', ExtraTreesClassifier())\n",
    "    ]),\n",
    "\n",
    "    'ridge_classifier': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('ridge', RidgeClassifier())\n",
    "    ]),\n",
    "\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "\n",
    "    'naive_bayes': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('nb', MultinomialNB())\n",
    "    ]),\n",
    "\n",
    "    'svm': Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "\n",
    "\n",
    "    'hard_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "      ('hard_model',VotingClassifier([('LR',LogisticRegression()),('ridge',RidgeClassifier()),('svm',SVC()),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='hard'))\n",
    "\n",
    "    ]),\n",
    "    'soft_model' : Pipeline([\n",
    "        ('vect', TfidfVectorizer()),\n",
    "        ('soft_model',VotingClassifier([('LR',LogisticRegression()),('svm',SVC(probability=True)),('et',ExtraTreesClassifier()),('nb',MultinomialNB())],voting='soft'))\n",
    "\n",
    "    ])\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "\n",
    "    # 텍스트 데이터를 TF-IDF 벡터로 변환\n",
    "    X_train_tfidf = pipeline.named_steps['vect'].fit_transform(X_train)\n",
    "    X_test_tfidf = pipeline.named_steps['vect'].transform(X_test)\n",
    "    \n",
    "    # 차원 출력\n",
    "    print(f\"{model_name} - X_train shape: {X_train_tfidf.shape}\")\n",
    "    print(f\"{model_name} - X_test shape: {X_test_tfidf.shape}\")\n",
    "    \n",
    "    # 모델 훈련\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # 교차검증 점수 계산\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    \n",
    "    # 성능 출력\n",
    "    train_score = pipeline.score(X_train, y_train)\n",
    "    test_score = pipeline.score(X_test, y_test)\n",
    "    print(f\"{model_name} Train set score: {train_score:.4f}\")\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\")\n",
    "    print(f\"{model_name} Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"{model_name} Cross-validation mean score: {cv_scores.mean():.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'{target}_{model_name}_tfidf_nonemasking_model.pkl')\n",
    "    joblib.dump(pipeline, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training logistic_regression...\n",
      "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
      "\n",
      "logistic_regression Best parameters found: {'lr__C': 1, 'lr__solver': 'liblinear', 'pca__n_components': 100, 'vect__max_features': 10000}\n",
      "logistic_regression Best cross-validation score: 0.8766\n",
      "logistic_regression Test set score: 0.8750\n",
      "\n",
      "Training svm...\n",
      "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n",
      "\n",
      "svm Best parameters found: {'pca__n_components': 100, 'svm__C': 10, 'svm__kernel': 'rbf', 'vect__max_features': 10000}\n",
      "svm Best cross-validation score: 0.7352\n",
      "svm Test set score: 0.7500\n",
      "\n",
      "Training random_forest...\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "\n",
      "random_forest Best parameters found: {'pca__n_components': 500, 'rf__max_depth': 20, 'rf__n_estimators': 50, 'vect__max_features': 30000}\n",
      "random_forest Best cross-validation score: 0.8608\n",
      "random_forest Test set score: 0.7500\n",
      "\n",
      "Training gradient_boosting...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "\n",
      "gradient_boosting Best parameters found: {'gb__learning_rate': 0.1, 'gb__max_depth': 3, 'gb__n_estimators': 100, 'pca__n_components': 500, 'vect__max_features': 20000}\n",
      "gradient_boosting Best cross-validation score: 0.7655\n",
      "gradient_boosting Test set score: 0.5000\n",
      "\n",
      "Training ada_boost...\n",
      "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
      "\n",
      "ada_boost Best parameters found: {'ada__learning_rate': 0.01, 'ada__n_estimators': 200, 'pca__n_components': 100, 'vect__max_features': 10000}\n",
      "ada_boost Best cross-validation score: 0.7835\n",
      "ada_boost Test set score: 0.6875\n",
      "\n",
      "Training xgboost...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "xgboost Best parameters found: {'pca__n_components': 100, 'vect__max_features': 30000, 'xgb__learning_rate': 0.2, 'xgb__max_depth': 3, 'xgb__n_estimators': 200}\n",
      "xgboost Best cross-validation score: 0.8449\n",
      "xgboost Test set score: 0.8750\n",
      "\n",
      "Training lightgbm...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "[LightGBM] [Info] Number of positive: 32, number of negative: 32\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1452\n",
      "[LightGBM] [Info] Number of data points in the train set: 64, number of used features: 64\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "\n",
      "lightgbm Best parameters found: {'lgb__learning_rate': 0.2, 'lgb__max_depth': 3, 'lgb__n_estimators': 50, 'pca__n_components': 1000, 'vect__max_features': 20000}\n",
      "lightgbm Best cross-validation score: 0.8903\n",
      "lightgbm Test set score: 0.7500\n",
      "\n",
      "Training catboost...\n",
      "Fitting 3 folds for each of 243 candidates, totalling 729 fits\n",
      "\n",
      "catboost Best parameters found: {'cat__depth': 5, 'cat__iterations': 50, 'cat__learning_rate': 0.1, 'pca__n_components': 100, 'vect__max_features': 30000}\n",
      "catboost Best cross-validation score: 0.8449\n",
      "catboost Test set score: 0.6250\n",
      "\n",
      "All models have been trained and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# XGBoost, LightGBM, CatBoost import\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/라벨링 데이터')\n",
    "\n",
    "data1 = pd.read_csv('masking_labeling_data.csv')\n",
    "data2 = pd.read_csv('masking_labeling_data2.csv')\n",
    "\n",
    "data1 = data1.rename(columns={'text_morphed_masked':'text_morphed_masked_'})\n",
    "data2 = data2.rename(columns={'text_morphed_masked':'text_morphed_masked_'})\n",
    "data1 = data1.rename(columns={'저격성 민원':'저격성민원_'})\n",
    "data2 = data2.rename(columns={'저격성 민원':'저격성민원_'})\n",
    "\n",
    "selected_columns1 = ['text_morphed_masked_', 'aggr', '욕설_모욕', '비꼼_시비', '성희롱', '요지불명', '저격성민원_']\n",
    "data1 = data1[selected_columns1]\n",
    "data2 = data2[selected_columns1]\n",
    "data1 = data1[:40]\n",
    "data2 = data2[data2.저격성민원_ == 1]                #독립변수 설정\n",
    "\n",
    "data3 = pd.concat([data1, data2])\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data3.iloc[:, 0]\n",
    "data_labeling = data3.loc[:, '저격성민원_']             #독립변수 설정\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_contents, data_labeling, stratify=data_labeling, test_size=0.2)\n",
    "\n",
    "# 파라미터 그리드 정의\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'lr__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'lr__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'svm': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'svm__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'svm__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'rf__n_estimators': [50, 100, 200],\n",
    "        'rf__max_depth': [None, 10, 20, 30]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'gb__n_estimators': [50, 100, 200],\n",
    "        'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'gb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'ada_boost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'ada__n_estimators': [50, 100, 200],\n",
    "        'ada__learning_rate': [0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'lgb__n_estimators': [50, 100, 200],\n",
    "        'lgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'lgb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'catboost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'pca__n_components': [100, 500, 1000],  # TruncatedSVD의 n_components 추가\n",
    "        'cat__iterations': [50, 100, 200],\n",
    "        'cat__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'cat__depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "    'svm': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'ada_boost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('ada', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'xgboost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('xgb', xgb.XGBClassifier(eval_metric='mlogloss'))\n",
    "    ]),\n",
    "    'lightgbm': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('lgb', lgb.LGBMClassifier())\n",
    "    ]),\n",
    "    'catboost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD()),  # TruncatedSVD의 n_components는 그리드 서치에서 조정됨\n",
    "        ('cat', cb.CatBoostClassifier(learning_rate=0.1, iterations=100, depth=6, verbose=0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # 그리드 서치 정의\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # 모델 훈련\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # 최적의 파라미터와 성능 출력\n",
    "    print(f\"\\n{model_name} Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 최적의 모델로 테스트 데이터 평가\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    model_save_path = os.path.join('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/', f'저격성민원_{model_name}_model_grid_search.pkl')     #독립변수 설정\n",
    "    joblib.dump(grid_search, model_save_path)\n",
    "\n",
    "print(\"All models have been trained and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      안녕 하 관리자 `지역` 문화센터 인근 학교 재직 교사 `지역` 문화센터 근방 신호...\n",
       "1      안녕 하 `지역` 아이 주민 에 인도 파손 되 넘어지 경험 있 인도 파손 부분 인도...\n",
       "2      `지역` `지역` `지역` 차선 도로 파손 되 매우 크 불편 겪 사진 첨부 지하철 ...\n",
       "3      `지역` 교차로 비보 호 좌회전 신호 있 좌회전 신호일 때 좌회전 도로 횡단보도 신...\n",
       "4      안녕 하 코로나 도둑 맞 같 현실 모든 가족 너무 화 세상에 있 답답 일단 국민 신...\n",
       "                             ...                        \n",
       "192    `지역` 빌 쓰레기 처리 대하 건의 사람 알 빌라 주민 만나 옮기 주시 하 궁 나 ...\n",
       "193    추석연 휴 친정 이틀 `지역` 빌라 차이 아직 덥 때 문 열 미치 알 사람 버리 쓰...\n",
       "194                 지나 달 차례 문의 드리 아직 방지 턱 설치 안 되 있 지나 치이\n",
       "195    군수 건축 과장 안녕 하 지나 일요일 오전 군청 발주 하 `지역` 농공 단지 앞 하...\n",
       "196    군도 호선 미개 설 공 촉구 건 대하 부여군 건설과 도로계 대답 시원 보충 질문 존...\n",
       "Name: text_morphed_masked_, Length: 395, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filepath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\n",
      "classpath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\rhinoMorph/lib/rhino.jar\n",
      "JVM is already started~\n",
      "RHINO started!\n",
      "filepath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\n",
      "classpath:  C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\rhinoMorph/lib/rhino.jar\n",
      "JVM is already started~\n",
      "RHINO started!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(morphed_file, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcp949\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# 데이터 분할\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], stratify\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# 파라미터 그리드 정의\u001b[39;00m\n\u001b[0;32m     84\u001b[0m param_grids \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogistic_regression\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvect__max_features\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;241m20000\u001b[39m, \u001b[38;5;241m30000\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    131\u001b[0m     }\n\u001b[0;32m    132\u001b[0m }\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import rhinoMorph\n",
    "\n",
    "# XGBoost, LightGBM, CatBoost import\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# 경로 설정\n",
    "data_path = 'C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/143.민원 업무 효율, 자동화를 위한 언어 AI 학습데이터/라벨링 데이터/'\n",
    "os.chdir(data_path)\n",
    "\n",
    "# 데이터 읽기\n",
    "data1 = pd.read_csv('공격성_민원_0.csv')\n",
    "data2 = pd.read_csv('공격성 민원 모음.csv')\n",
    "\n",
    "# 데이터 전처리\n",
    "data2 = data2.drop('Unnamed: 0', axis=1)\n",
    "data2.rename(columns={\"민원내용\": 'title_and_contents'}, inplace=True)\n",
    "data1.rename(columns={\"저격성 민원\": '저격성민원'}, inplace=True)\n",
    "\n",
    "# 데이터 병합\n",
    "data3 = pd.concat([data1, data2])\n",
    "data4 = data3[['title_and_contents', 'aggr']].reset_index(drop=True)\n",
    "\n",
    "# 텍스트와 라벨 분리\n",
    "data_contents = data4['title_and_contents']\n",
    "data_labeling = data4['aggr']\n",
    "\n",
    "# 형태소 분석\n",
    "rn = rhinoMorph.startRhino()\n",
    "\n",
    "def morph_and_label(contents, labels):\n",
    "    location_data = pd.read_csv('C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/행정구역 데이터/file/행정구역명리스트.csv')\n",
    "    location_list = list(location_data.loc[:, 'combined'])\n",
    "\n",
    "    rn = rhinoMorph.startRhino()\n",
    "\n",
    "    morphed_data_list = []\n",
    "    for data_each in data_contents:\n",
    "        morphed_data_each, poses = rhinoMorph.wholeResult_list(rn, str(data_each),pos=['NNG', 'NNP', 'VV', 'VA', 'XR', 'IC', 'MM', 'MAG', 'MAJ'])\n",
    "        # print(morphed_data_each)\n",
    "        # print(poses)\n",
    "        # 형태소 분석된 데이터에서 NNP에 대해 이름 정규 표현식 적용 및 지역명 체크\n",
    "        for j, item in enumerate(poses):\n",
    "            if item == 'NNP':\n",
    "                word = morphed_data_each[j]\n",
    "                \n",
    "                # 지역명 리스트에 있는지 체크\n",
    "                if word in location_list:\n",
    "                    morphed_data_each[j] = '`지역`'\n",
    "            \n",
    "        joined_data_each = ' '.join(morphed_data_each)\n",
    "        if joined_data_each:\n",
    "            morphed_data_list.append(joined_data_each)\n",
    "        else : \n",
    "            morphed_data_list.append(' ')\n",
    "\n",
    "morphed_data = morph_and_label(data_contents, data_labeling)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "morphed_df = pd.DataFrame(morphed_data, columns=['text', 'label'])\n",
    "\n",
    "# 데이터 저장\n",
    "morphed_file = 'ratings_morphed.txt'\n",
    "morphed_df.to_csv(morphed_file, index=False, encoding='cp949')\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv(morphed_file, encoding='cp949')\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], stratify=data['label'], test_size=0.2)\n",
    "\n",
    "# 파라미터 그리드 정의\n",
    "param_grids = {\n",
    "    'logistic_regression': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'lr__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'lr__solver': ['liblinear', 'lbfgs']\n",
    "    },\n",
    "    'svm': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'svm__C': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'svm__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'rf__n_estimators': [50, 100, 200],\n",
    "        'rf__max_depth': [None, 10, 20, 30]\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'gb__n_estimators': [50, 100, 200],\n",
    "        'gb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'gb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'naive_bayes': {\n",
    "        'vect__max_features': [10000, 20000, 30000]\n",
    "    },\n",
    "    'ada_boost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'ada__n_estimators': [50, 100, 200],\n",
    "        'ada__learning_rate': [0.001, 0.01, 0.1]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'xgb__n_estimators': [50, 100, 200],\n",
    "        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'xgb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'lightgbm': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'lgb__n_estimators': [50, 100, 200],\n",
    "        'lgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'lgb__max_depth': [3, 5, 7]\n",
    "    },\n",
    "    'catboost': {\n",
    "        'vect__max_features': [10000, 20000, 30000],\n",
    "        'cat__iterations': [50, 100, 200],\n",
    "        'cat__learning_rate': [0.01, 0.1, 0.2],\n",
    "        'cat__depth': [3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 파이프라인 정의\n",
    "pipelines = {\n",
    "    'logistic_regression': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('lr', LogisticRegression())\n",
    "    ]),\n",
    "    'svm': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('svm', SVC())\n",
    "    ]),\n",
    "    'random_forest': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('rf', RandomForestClassifier())\n",
    "    ]),\n",
    "    'gradient_boosting': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('gb', GradientBoostingClassifier())\n",
    "    ]),\n",
    "    'ada_boost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('ada', AdaBoostClassifier())\n",
    "    ]),\n",
    "    'xgboost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('xgb', xgb.XGBClassifier(eval_metric='mlogloss'))\n",
    "    ]),\n",
    "    'lightgbm': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('lgb', lgb.LGBMClassifier())\n",
    "    ]),\n",
    "    'catboost': Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('pca', TruncatedSVD(n_components=1000)),\n",
    "        ('cat', cb.CatBoostClassifier(learning_rate=0.1, iterations=100, depth=6, verbose=0))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# 모델 훈련 및 평가\n",
    "model_save_path = 'C:/Users/USER/Desktop/핵심역량 프로젝트/데이터/모델/'\n",
    "os.chdir(model_save_path)\n",
    "\n",
    "for model_name, pipeline in pipelines.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    # 그리드 서치 정의\n",
    "    grid_search = GridSearchCV(pipeline, param_grids[model_name], cv=3, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # 모델 훈련\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # 최적의 파라미터와 성능 출력\n",
    "    print(f\"\\n{model_name} Best parameters found: {grid_search.best_params_}\")\n",
    "    print(f\"{model_name} Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # 최적의 모델로 테스트 데이터 평가\n",
    "    test_score = grid_search.score(X_test, y_test)\n",
    "    print(f\"{model_name} Test set score: {test_score:.4f}\\n\")\n",
    "    \n",
    "    # 모델 저장\n",
    "    joblib.dump(grid_search, f'{model_name}_model_grid_search.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
